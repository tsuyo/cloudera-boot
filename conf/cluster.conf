name: cloudera-boot-cluster

provider {
    type: aws
    accessKeyId: ${?AWS_ACCESS_KEY_ID}
    secretAccessKey: ${?AWS_SECRET_ACCESS_KEY}
    region: ap-northeast-1
    subnetId: subnet-ce444a97
    securityGroupsIds: sg-65014801
    instanceNamePrefix: cloudera-boot
}

ssh {
    username: centos
    privateKey: ${?SSH_PRIVATE_KEY} # with an absolute path to .pem file
}

common {
    image: ami-4dd5522b # CentOS Linux 7 x86_64 HVM EBS
    tags {
        owner: tsuyo
    }
    # Turning off mountAllUnmountedDisks is necessary for CDSW to work, as CDSW handles
    # this step on its own
    normalizationConfig {
        mountAllUnmountedDisks: false
    }
}

instances {
    m4x: ${common} {
        type: m4.xlarge
    }
    m44x: ${common} {
        type: m4.4xlarge
    }
}

cloudera-manager {
    instance: ${instances.m4x} {
        tags {
            application: "Cloudera Manager 6"
        }
    }
    repository: "https://archive.cloudera.com/cm6/6.0/redhat7/yum/"
    repositoryKeyUrl: "https://archive.cloudera.com/cm6/6.0/redhat7/yum/RPM-GPG-KEY-cloudera"
    configs {
        CLOUDERA_MANAGER {
            custom_banner_html: "Cloudera Boot Cluster"
            enable_embedded_db_check: false
            enable_api_debug: true
        }
    }

    # jdk 1.8 is already installed in instance bootstrap
    # javaInstallationStrategy: NONE

#    csds: [
#        "https://archive.cloudera.com/cdsw/1/csd/CLOUDERA_DATA_SCIENCE_WORKBENCH-1.3.1.jar"
#    ]
    enableEnterpriseTrial: true
}

cluster {
    products {
      CDH: 6
#      CDSW: 1.3.1
    }
    parcelRepositories: [
        "https://archive.cloudera.com/cdh6/6.0/parcels/",
        "https://archive.cloudera.com/cdsw/1/parcels/1.3.1/"
    ]
    # services: [HDFS, YARN, ZOOKEEPER, CDSW, HIVE, HUE, OOZIE, SPARK_ON_YARN]
    # services: [HDFS, YARN, ZOOKEEPER, CDSW, SPARK_ON_YARN]
    services: [HDFS, YARN, ZOOKEEPER, SPARK_ON_YARN]

    masters {
        count: 1
        instance: ${instances.m4x}
        roles {
            HDFS: [NAMENODE, SECONDARYNAMENODE]
            YARN: [RESOURCEMANAGER, JOBHISTORY]
            # HIVE: [HIVESERVER2, HIVEMETASTORE]
            # HUE: [HUE_SERVER]
            # OOZIE: [OOZIE_SERVER]
            SPARK_ON_YARN: [SPARK_YARN_HISTORY_SERVER]
        }
    }

    workers {
        count: 3
        minCount: 3
        instance: ${instances.m4x}
        roles {
            HDFS: [DATANODE]
            YARN: [NODEMANAGER]
            ZOOKEEPER: [SERVER]
        }
    }
#    postCreateScripts: ["""#!/bin/sh
#sudo -u hdfs hdfs dfs -mkdir /user/"""${ssh.username}"""
#sudo -u hdfs hdfs dfs -chown """${ssh.username}""":"""${ssh.username}""" /user/"""${ssh.username}"""
#    """
#    ]

#    cdsw-masters {
#        count: 1
#        instance: ${instances.m44x} {
#            # Disks for aws
#            rootVolumeSizeGB: 100
#            ebsVolumeCount : 3
#            ebsVolumeSizeGiB: 1000
#            ebsVolumeType: gp2
#
#            bootstrapScripts: [ """#!/bin/sh
#
#            # Mount one volume for application data
#            device="/dev/xvdh"
#            mount="/var/lib/cdsw"
#
#            echo "Making file system"
#            mkfs.ext4 -F -E lazy_itable_init=1 "$device" -m 0
#
#            echo "Mounting $device on $mount"
#            if [ ! -e "$mount" ]; then
#              mkdir -p "$mount"
#            fi
#
#            mount -o defaults,noatime "$device" "$mount"
#            echo "$device $mount ext4 defaults,noatime 0 0" >> /etc/fstab
#
#            exit 0
#            """ ]
#        }
#
#        roles {
#            HDFS: [GATEWAY]
#            YARN: [GATEWAY]
#            CDSW: [CDSW_MASTER, CDSW_APPLICATION, CDSW_DOCKER]
#            SPARK_ON_YARN: [GATEWAY]
#        }
#
#        configs {
#            CDSW {
#                CDSW_DOCKER {
#                    "cdsw.docker.devices.config": "/dev/xvdf /dev/xvdg" # related to the ebs configuration above
#                }
#            }
#        }
#    }
}
